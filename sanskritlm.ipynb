{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd198c78",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-01T19:31:11.747848Z",
     "iopub.status.busy": "2024-11-01T19:31:11.747513Z",
     "iopub.status.idle": "2024-11-01T19:31:12.488941Z",
     "shell.execute_reply": "2024-11-01T19:31:12.488204Z"
    },
    "papermill": {
     "duration": 0.74723,
     "end_time": "2024-11-01T19:31:12.491276",
     "exception": false,
     "start_time": "2024-11-01T19:31:11.744046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ccc825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T19:31:12.496960Z",
     "iopub.status.busy": "2024-11-01T19:31:12.496553Z",
     "iopub.status.idle": "2024-11-01T19:31:28.802060Z",
     "shell.execute_reply": "2024-11-01T19:31:28.801071Z"
    },
    "papermill": {
     "duration": 16.310956,
     "end_time": "2024-11-01T19:31:28.804363",
     "exception": false,
     "start_time": "2024-11-01T19:31:12.493407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Loss: 3.6741\n",
      "Epoch: 1, Average Loss: 2.9680\n",
      "Epoch: 2, Batch: 0, Loss: 2.3347\n",
      "Epoch: 2, Average Loss: 2.0750\n",
      "Epoch: 3, Batch: 0, Loss: 1.7613\n",
      "Epoch: 3, Average Loss: 1.7535\n",
      "Epoch: 4, Batch: 0, Loss: 1.6214\n",
      "Epoch: 4, Average Loss: 1.5332\n",
      "Epoch: 5, Batch: 0, Loss: 1.4688\n",
      "Epoch: 5, Average Loss: 1.4034\n",
      "Epoch: 6, Batch: 0, Loss: 1.3323\n",
      "Epoch: 6, Average Loss: 1.3369\n",
      "Epoch: 7, Batch: 0, Loss: 1.2776\n",
      "Epoch: 7, Average Loss: 1.2473\n",
      "Epoch: 8, Batch: 0, Loss: 1.1752\n",
      "Epoch: 8, Average Loss: 1.1342\n",
      "Epoch: 9, Batch: 0, Loss: 1.0816\n",
      "Epoch: 9, Average Loss: 1.0164\n",
      "Epoch: 10, Batch: 0, Loss: 0.9564\n",
      "Epoch: 10, Average Loss: 0.9061\n",
      "\n",
      "Generated Sanskrit text:\n",
      " hadgīmaprāya-kṣete kṛtsatsa athaḥ yāḥ pretha tha śiṣeta prama vyā dha  pu-kṛtrupru-kutretre tretrupudrupuyuyutrutruyu-kṣyuve  satsamavavaḥ maḥ utāyutsa iṣyu-kṣetrṇadreṇa trutruvyuyūḍhyuyuyupuputru-kṣe śrutruyutāyuputāṁ savadretsa-kṣyuretrute yetreṇavyuyūḍhave da va drutsaḥ yūḍhyūḍhṛthavava dha da  dha truputrṁ dharetā drīma kurutrururmamarurutaruyuvyutrutretru-kṣe savyutāṁ vyutru-puyūḍhaṣetsa yutretruyu-kṣetrutrutrutrśiṣetsa śruruyuyu-kuyuṁ savave śiṣyuyuṇavamavyūḍhā utsavetrutreta ta savetre dr\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class CharacterTokenizer:\n",
    "    def __init__(self):\n",
    "        self.char_to_idx = {}\n",
    "        self.idx_to_char = {}\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def fit(self, text):\n",
    "        unique_chars = sorted(set(text))\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "        self.idx_to_char = {idx: char for idx, char in enumerate(unique_chars)}\n",
    "        self.vocab_size = len(unique_chars)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return [self.char_to_idx[char] for char in text]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.idx_to_char[idx] for idx in indices])\n",
    "\n",
    "class SanskritDataset(Dataset):\n",
    "    def __init__(self, text, sequence_length, tokenizer):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.text = text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoded_text = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "        \n",
    "        if len(self.encoded_text) < self.sequence_length:\n",
    "            raise ValueError(\"The sequence length is longer than the text length. \"\n",
    "                             \"Please provide a shorter sequence length or a longer text.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.encoded_text) - self.sequence_length)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.encoded_text[idx:idx + self.sequence_length]\n",
    "        y = self.encoded_text[idx + 1:idx + self.sequence_length + 1]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class SanskritGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "def train_model(model, train_loader, optimizer, device, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(data, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch: {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    batch_size = 32\n",
    "    block_size = 64\n",
    "    n_embd = 384\n",
    "    n_head = 6\n",
    "    n_layer = 6\n",
    "    dropout = 0.2\n",
    "    learning_rate = 3e-4\n",
    "    epochs = 10\n",
    "    \n",
    "    # Load Sanskrit text data\n",
    "    text = \"prāptarājyasya rāmasya rākṣasānāṃ vadhe kṛte atha śrīmadbhagavadgītāyāḥ prathamo'dhyāyaḥ dhṛtarāṣṭra uvāca:dharma-kṣetre kuru-kṣetre samavetā yuyutsavaḥ vyūḍhāṁ drupada-putreṇa tava śiṣyeṇa dhīmatā\"  # Replace with full Sanskrit text corpus as needed\n",
    "    \n",
    "    # Initialize tokenizer and encode text\n",
    "    tokenizer = CharacterTokenizer()\n",
    "    tokenizer.fit(text)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = SanskritDataset(text, block_size, tokenizer)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SanskritGPT(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        n_embd=n_embd,\n",
    "        block_size=block_size,\n",
    "        n_head=n_head,\n",
    "        n_layer=n_layer,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_loader, optimizer, device, epochs)\n",
    "    \n",
    "    # Generate sample text\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    generated_indices = model.generate(context, max_new_tokens=500, temperature=0.8)[0].tolist()\n",
    "    generated_text = tokenizer.decode(generated_indices)\n",
    "    print(\"\\nGenerated Sanskrit text:\")\n",
    "    print(generated_text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21.022979,
   "end_time": "2024-11-01T19:31:30.028672",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-01T19:31:09.005693",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
